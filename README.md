# StreamLIC-Quantized-Implementation
Pytorch implementation of StreamLIC in VCIP2025 paper "StreamLIC: A Lightweight Learned Image Compression  Model for Stream Processing on FPGA"



## Features
*   **Sparsity:** Uses Fine-Grained Pruning (FGPConv) to reduce computational complexity.
*   **Quantization:** Full Quantization-Aware Training (QAT) support for weights and activations, simulating fixed-point hardware behavior.
*   **Adaptive PDF Table:** Replaces standard continuous probability models with `RectifiedPDF`, a discrete lookup-table-based approach for more accurate probability modeling.
*   **Parameter Export:** Utilities to export integerized parameters (weights, biases, scale factors) for HLS/FPGA implementation.

## Requirements

*   Python 3.x
*   [PyTorch](https://pytorch.org/)
*   [CompressAI](https://github.com/InterDigitalInc/CompressAI)


## Model Architecture

The model follows a hyperprior architecture with autoregressive context (AR). It utilizes custom layers found in `qat_utils.py`:
*   **Conv2dQ / Conv2dUpsample_Q:** Quantized convolution and upsampling.
*   **LAGC_Q:** Locally Adaptive Gain Control with quantization.
*   **FGPConv_Q:** Fine-Grained Pruning Convolution for sparse layers.

## Training Workflow

The training strategy is designed to transition the model from a continuous floating-point representation to a fixed-point, sparse, integer-aligned representation.

### 1. AUN / Initial Training
Start with standard training using additive uniform noise (AUN) for quantization simulation. Ensure QAT and Masking are disabled initially to let the model converge.

```python
model = ARLiteSparse_Q()
model.train()
model.toggle_quantization(False)
model.toggle_mask(False)
# ... Run standard training loop ...
```

### 2. Sparsity Masking
Once the model has converged, calculate the sparsity masks based on activation statistics and enable them.

```python
# Calculate masks based on statistics collected
model.set_all_mask() 

# Enable the mask (sparse inference simulation)
model.toggle_mask(True) 
# ... Continue training to recover accuracy ...
```

### 3. Quantization-Aware Training (QAT)
Enable quantization for weights and activations. This simulates the rounding effects of fixed-point arithmetic.

```python
model.toggle_quantization(True)
# ... Continue training ...
```

### 4. Hard Quantization (Fix Encoder)
To prepare for accurate entropy modeling, freeze the encoder parameters (`g_a`, `h_a`). This stabilizes the latent representation.

```python
model.fix_encoder_flag(True)
# ... Continue training (Decoder and Entropy parameters adapt) ...
```

### 5. PDF Training (Probability Modeling)
Finally, switch to `RectifiedPDF` training. This replaces the `GaussianConditional` and `EntropyBottleneck` with discrete lookup tables compatible with hardware implementation.

**Note:** You must initialize the PDF tables before toggling the flag.

```python
model.init_pdf_table()
model.toggle_pdf_training_flag(True)
# ... Final fine-tuning ...
```

## Key API Functions

### `toggle_quantization(qat_flag: bool)`
Switches the layers between floating-point mode and Quantization-Aware mode.
*   `False`: Uses standard floating-point operations.
*   `True`: Simulates int-arithmetic using Straight-Through Estimator (STE) rounding and clamped ranges.

### `toggle_mask(flag: bool)`
Toggles the application of the sparsity mask in `FGPConv_Q` layers.
*   `True`: Weights are masked (multiplied by 0 or 1), enforcing sparsity.
*   `False`: Weights are dense.

### `set_all_mask()`
Computes the sparsity mask for `FGPConv_Q` layers based on the magnitude of weights and input statistics gathered during forward passes. This should be called before enabling the mask permanently.

### `fix_encoder_flag(flag: bool)`
Freezes the gradients for the analysis transforms (`g_a`, `h_a`). This is essential during the final stages of QAT to ensure the latent codes generated by the encoder remain stable while the decoder and entropy model fine-tune.

### `toggle_pdf_training_flag(flag: bool)`
Enables the training of the `RectifiedPDF` modules.
*   When `True`: The model calculates likelihoods using discrete lookup tables (`rectified_cdf_y`, `rectified_cdf_z`) rather than continuous Gaussian/Factorized models. This ensures that the training loss matches the bitstream length produced during integer inference.

## Exporting for Inference

After the training pipeline is complete, you can export the quantized parameters (integer weights, biases, shift values, and scale multipliers) suitable for implementation in C++, HLS, or Verilog.

```python
import torch

# Ensure model is in eval mode and on CPU for export
model.eval()
model.cpu()

# Export parameters
quantized_params = model.export_quantized_parameters()

# Example: Accessing parameters for a specific layer
# print(quantized_params['g_a.0'].keys())
# Output: dict_keys(['weight', 'bias', 'scale_param', 'scale_shift', ...])

# Save to a file for hardware loading
torch.save(quantized_params, "quantized_model_params.pth")
```

The exported dictionary contains:
*   `weight`: Integer weights (`int32`).
*   `bias`: Integer biases (`int32`).
*   `scale_param` & `scale_shift`: Fixed-point multipliers used to rescale activations between layers to maintain precision.
*   `mask`: (For sparse layers) The binary mask.

---
